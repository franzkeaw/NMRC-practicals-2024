{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a perceptron\n",
    "\n",
    "We consider again a neuron which receives two inputs $x_1$ and $x_2$. You should be already familiar with the following equations, but we recap them here for clarity. The neuron combines the inputs $x_i$ as \n",
    "\n",
    "\\begin{equation}\n",
    "s = \\sum_{i=1}^2 w_i x_i + b\n",
    "\\end{equation}\n",
    "\n",
    "And then passes this value through and activation function $f$ to compute the output\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} = f(s)\n",
    "\\end{equation}\n",
    "\n",
    "Before proceeding, notice that we are using a slightly different notation with respect to the previous lectures. This is done in order to maintain consistency with the notation used in the lecture on backpropagation, which deals with how neural networks are trained, and is thus more closely releated with the material here. Here $x_i$ is the $i^{th}$ input, $w_i$ is the synaptic weight of the $i^{th}$ input, $b$ is the bias, $s$ is the weighted sum of inputs computed at the neuron, and $\\hat{y}$ is the output (i.e. the activation) of the neuron, after the weighted sum $s$ has been passed through the transfer function $f$. Also notice that the bias now appears with a positive sign ($+b$ instead of $-b$): this, however, should not confuse you, since from a modeling point of view the bias is just a constant that allows to move the activation threshold of our neuron, so this is just a matter of notation (if we have $+b$, increasing the bias will move the threshold up, if we have $-b$ increasing the bias will move the threshold down)\n",
    "\n",
    "In the previous practical we were able to manually adjust the weights to get the right output, by a sort of trial and error procedure. In this tutorial we are going to understand a principled way by which synaptic connections can be updated such that the neuron can produce the desired output. For this to work, we clearly need to know what this desired output is: we call this desired output the _target_ $y$. This allows us to compute an error, which measures the mismatch between the output of our neuron, and our desired output. The error (more commonly referred to as the _loss_ in the machine learning world) can be defined in various ways. Here, we use the following\n",
    "\n",
    "\\begin{equation}\n",
    "L = \\frac{1}{2}(\\hat{y}-y)^2\n",
    "\\end{equation}\n",
    "\n",
    "Our neuron is going to implement the `hardlim` transfer function.\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} = f(s) = \n",
    "\\left\\{\\begin{matrix}\n",
    "1 & \\mathrm{if} \\, s>0\\\\ \n",
    "0 & \\mathrm{otherwise}\n",
    "\\end{matrix}\\right.\n",
    "\\end{equation}\n",
    "\n",
    "Equipped with this function, this simple network corresponds to one of the very first neural networks ever introduced, the so-called *perceptron*. This neuron behaves as a *classifier*: it either responds (i.e. it outputs 1) or does nothing (i.e. it outputs 0), which means it is ideal for grouping input into two classes. \n",
    "\n",
    "We can think of the inputs $x_1$ and $x_2$ as representing some feature of the environment, such as the distance and the aggressiveness of an animal present in the visual field. Based on these inputs, the neuron can 'decide' whether the situation is dangerous or not. \n",
    "\n",
    "## Part 1 - The perceptron learning rule\n",
    "\n",
    "We are now going to try and understand how we can change the synaptic weights of our perceptron in a principled way. For this part, we focus on a single input $x$ and a single weight $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Suppose you have the following values for the input $x$, the output $\\hat{y}$, and the target $y$.\n",
    "\\begin{align}\n",
    "x &= 0.5 \\\\\n",
    "\\hat{y} &= 1 \\\\\n",
    "y &= 0 \\\\\n",
    "\\end{align}\n",
    "In which direction should the weight $w$ change such that our output becomes closer to the desired output? i.e. should $w$ increase ($\\Delta w > 0$) or decrease ($\\Delta w < 0$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 1\n",
    "[your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Consider now this set of values:\n",
    "\\begin{align}\n",
    "x &= -0.5 \\\\\n",
    "\\hat{y} &= 1 \\\\\n",
    "y &= 0 \\\\\n",
    "\\end{align}\n",
    "In which direction should the weight change this time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 2\n",
    "[your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Consider\n",
    "\\begin{align}\n",
    "x &= 0.5 \\\\\n",
    "\\hat{y} &= 0 \\\\\n",
    "y &= 1 \\\\\n",
    "\\end{align}\n",
    "In which direction should the weight change this time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 3\n",
    "[your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "Consider\n",
    "\\begin{align}\n",
    "x &= -0.5 \\\\\n",
    "\\hat{y} &= 0 \\\\\n",
    "y &= 1 \\\\\n",
    "\\end{align}\n",
    "In which direction should the weight change this time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 4\n",
    "[your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably realized that the sign of the input $x$ matters for how we should change the weight (if not, go back and check your answers again!). And of course the difference between output and target $e=(y-\\hat{y})$ also matters. The cases that were presented above can be summarized in the following table: \n",
    "\n",
    "| $e$    | $x$      | $\\Delta w$|\n",
    "| ----------:|:------:| -----:|\n",
    "| > 0         | > 0    |    ?   |\n",
    "| > 0         | < 0    |    ?   |\n",
    "| < 0         | > 0    |    ?   |\n",
    "| < 0         | < 0    |    ?   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Complete the table in the answer cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 5\n",
    "| $e$    |$x$      | $\\Delta w$|\n",
    "| ----------:|:------:| -----:|\n",
    "| > 0         | > 0    |       |\n",
    "| > 0         | < 0    |       |\n",
    "| < 0         | > 0    |       |\n",
    "| < 0         | < 0    |       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "So, the table tells us in which direction we should change $w$ given the signs of $e$ and $x$. What function of $f(e, x)$ achieves this behavior?\n",
    "\n",
    "_Hint:_ try $\\Delta w = e + x$. Do you get the desired behavior?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 6\n",
    "[your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were able to answer the previous question correctly, you came to the conclusion that $\\Delta w = e \\times x = (y-\\hat{y})x$. While this may not seem like much, you have actually just deduced the __perceptron learning rule__, as you can see on [Wikipedia](https://en.wikipedia.org/wiki/Perceptron). \n",
    "\n",
    "We forgot one thing though: the bias! We need to learn that too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "Fill out the update table for the bias in the answer cell below.\n",
    "\n",
    "_Hint:_ think carefully whether the bias update should depend on $e$ and/or on $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 7\n",
    "| $e$    | $b$      | $\\Delta b$|\n",
    "| ----------:|:------:| -----:|\n",
    "| > 0         | > 0    |       |\n",
    "| > 0         | < 0    |       |\n",
    "| < 0         | > 0    |       |\n",
    "| < 0         | < 0    |       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You hopefully came to the conclusion that we need to change the bias based only on $e$, and in fact it turns out that in the perceptron, we generally use $\\Delta b = e = (y-\\hat{y})$. We are now fully equipped to _train_ our neuron to classify data. \n",
    "\n",
    "There's one last element that we are missing, which is the *learning rate*. Essentially, we can modulate *how much* we change our parameters (the weights and the bias) for each new data point that we observe. We want to avoid that when see a new data point we drastically change our parameters (potentially erasing what we learned before), rather we want to make small adjustments at every new observation. The learning rate, which we call $\\alpha$, is generally (but not necessarily) treated as a *hyperparameter*, meaning that we don't learn it from our data, but we manually set it to a value. So our full parameter update becomes: \n",
    "\n",
    "\\begin{align}\n",
    "w &= w + \\alpha \\Delta w = w + \\alpha (y-\\hat{y})x \\\\\n",
    "b &= b + \\alpha \\Delta b = b + \\alpha (y-\\hat{y})\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "## Part 2 - Training a perceptron\n",
    "\n",
    "Let's first generate some data: run the cell below which will generate and plot data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATcAAAEhCAYAAAAaree0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nObiZWSk5uJlZKf1/0AvTIkJuDcMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 320x320 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi']= 80\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "cluster_std = [[0.2, 0.2], [0.2, 0.2]]\n",
    "centers     = [[1, 2], [2, 1]]\n",
    "X, y = make_blobs(n_samples=50, n_features=2, cluster_std=cluster_std,\n",
    "                  centers=centers,shuffle=True)\n",
    "f, ax = plt.subplots(1, 1, figsize=[4, 4])\n",
    "ax.scatter(X[y == 0, 0], X[y == 0, 1], color='g')\n",
    "ax.scatter(X[y == 1, 0], X[y == 1, 1], color='r')\n",
    "ax.set_xlabel('p1')\n",
    "ax.set_ylabel('p2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above, every dot represents a combination of values of $x_1$ and $x_2$. The color indicated whether our neuron should output a 0 or a 1 for that combination. To continue the analogy of $x_1$ and $x_2$ representing features of a visual scene, the red dots correspond to situations which our neuron should classify as dangerous.\n",
    "\n",
    "Now we are good to go. In this simulation, we are going to train our neuron using the equations for $\\Delta w$ and $\\Delta b$ you derived. This is going to work as follows: we loop through every point in the dataset, we know what our neuron outputs ($\\hat{y}$), and we know what the output _should_ be $(y)$. This means that for every data point $x$ we can compute $\\Delta w$ and $\\Delta b$ and update our synapses and bias as\n",
    "\n",
    "\\begin{align}\n",
    "w_i &= w_i + \\alpha \\Delta w_i = w_i + \\alpha(y-\\hat{y})x_i \\\\\n",
    "b &= b + \\alpha \\Delta b = b + \\alpha(y-\\hat{y})\n",
    "\\end{align}\n",
    "\n",
    "Every time we observe a new data point, we can update our parameters accordingly. In the simulation, we loop over our dataset and for every data point update weights and bias. After every update we plot the _decision boundary_  and the _training accuracy_ (the number of points in our dataset that are classified correctly given the current weight and bias). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Run the cell below, press 'Initialize' to initialize the simulation and 'Train' to launch the training procedure. Does the neuron learn the correct classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d498d883be4599b27b1f9fbfa42164"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b32296043514f7283b77c4aabba9887"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils_assignment_3 import Neuron, mlp_simulation\n",
    "sim = mlp_simulation(X, y, learning_rate=0.00001)\n",
    "sim.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 1\n",
    "[your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "We now change the distribution of our data: run the cell below and initialize the simulation. Do you think that the classification problem became easier or harder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_std = [[1, 0.3], [1, 0.3]]\n",
    "centers     = [[1, 2], [2, 1]]\n",
    "X, y = make_blobs(n_samples=50, n_features=2, cluster_std=cluster_std,\n",
    "                  centers=centers,shuffle=True)\n",
    "sim = mlp_simulation(X, y, learning_rate=0.00001)\n",
    "sim.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Answer 2\n",
    "[your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "You can see that if your run the simulation multiple times, it will converge to different solutions. So given the same classification problem and the same inputs we will not get the same weights. What are 2 sources of this variability? (hint: think about how the network initializes and trains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Answer 3\n",
    "[your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "What could be a weakness of this learning algorithm? Think about how the network updates its weight and bias!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 4\n",
    "[your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segway to the next assignment:\n",
    "As in the previous assigment, we are dealing with a _linear classifier_. Let's generate some more complicated data distribution, and see how our perceptron fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, make_circles\n",
    "X, y = make_moons(n_samples=50, shuffle=True, noise=0.05)\n",
    "\n",
    "sim = mlp_simulation(X, y, learning_rate=0.00001)\n",
    "sim.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
